# This is a Databricks asset bundle definition for asset_bundles_template_test_1.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
bundle:
  name: source-A-bundle

include:
  - resources/*.yml

sync:
  exclude:
    - .vscode
    - .gitignore
    - fixtures

variables:
  prod_path:
    description: 'Folder in which bundle will be deployed when approved for production (-t prod)'
    default: /Production/${bundle.name}/

  cluster_id_testing_cluster:
    description: id of cluster used for (nutter) testing
    default: 0316-094940-oenlls44

targets:
  dev:
    mode: development
    default: true
    workspace:
      host: https://adb-1909067673949101.1.azuredatabricks.net/

    # sync:
    #   include:
    #     - src/*
    #     - resources/*.yml
  prod:
    # We use 'mode: production' to indicate this is a production deployment.
    # Doing so enables strict verification of the settings below.
    mode: production
    workspace:
      host: https://adb-1909067673949101.1.azuredatabricks.net/
      # We only have a single deployment copy for production, 
      # so we use a shared path.
      root_path: ${var.prod_path}
    run_as:
      # (see https://docs.databricks.com/dev-tools/bundles/permissions.html).
      # user_name: bbluestone@outlook.com
      service_principal_name: 724b31cb-bccb-42dc-b251-3bbdbcf86df4

    sync:
      include:
        - src/*
        - resources/*.yml
        
resources:
  jobs:
    wheel-job:
      name: wheel-job
      tasks:
        - task_key: wheel-task
          new_cluster:
            spark_version: 13.3.x-scala2.12
            node_type_id: Standard_D3_v2
            data_security_mode: USER_ISOLATION
            num_workers: 1
          python_wheel_task:
            entry_point: main
            package_name: my_wheel
          libraries:
            - whl: ./dist/*.whl

    # resources:
    #   jobs:
    #     source_A_etl:
    #       job_clusters:
    #         - job_cluster_key: job_cluster_category_1
    #           new_cluster:
    #             spark_version: 13.3.x-scala2.12
    #             node_type_id: Standard_D5_v2
    #             autoscale:
    #               min_workers: 1
    #               max_workers: 1
    #       tasks:
    #         - task_key: upload_to_bronze
    #           job_cluster_key: job_cluster_category_1
    #           notebook_task:
    #             notebook_path: ./src/source_A_1_raw_to_bronze_notebook.py

    #         - task_key: bronze_to_silver
    #           job_cluster_key: job_cluster_category_1
    #           notebook_task:
    #             notebook_path: ./src/source_A_2_bronze_to_silver_notebook.py

    #         - task_key: silver_to_gold
    #           job_cluster_key: job_cluster_category_1
    #           notebook_task:
    #             notebook_path: ./src/source_A_3_silver_to_gold_notebook.py